iterations	learning rate	removed stopwords	training accuracy	testing accuracy
10			1.0				no					90.06				86.40
10			1.0				yes					96.98				89.75
10			0.1				no					88.56				85.36
10			0.1				yes					97.98				89.86
10			0.01			no					90.93				85.98
10			0.01			yes					96.54				90.79
10			0.001			no					91.36				76.15
10			0.001			yes					97.19				88.91
10			5.0				no					90.06				86.40
10			5.0				yes					98.92				89.54
10			10.0			no					90.06				79.71
10			10.0			yes					95.90				89.96
100			1.0				no					100.00				88.08
100			1.0				yes					100.00				90.79
100			0.1				no					100.00				88.08
100			0.1				yes					100.00				89.54
100			5.0				no					100.00				88.28
100			5.0				yes					100.00				90.79
20			0.1				no					92.22				87.03
20			0.1				yes					100.00				91.21
20			1.0				no					96.33				87.45
20			1.0				yes					100.00				88.08
50			0.01			no					98.70				88.70
50			0.01			yes					100.00				89.96
50			0.1				no					99.35				87.66
50			0.1				yes					100.00				88.91
50			1.0				no					99.35				87.87
50			1.0				yes					100.00				90.79
50			5.0				no					99.14				87.87
50			5.0				yes					100.00				90.79
25			0.001			no					95.68				86.19
25			0.001			yes					99.78				88.91
20			0.5				no					92.66				87.45
20			0.5				yes					100.00				90.79
50			0.25			no					99.35				87.87
50			0.25			yes					100.00				88.28
20			2.5				no					95.46				87.45
20			2.5				yes					95.46				88.91
100			10.0			no					100.00				88.28
100			10.0			yes					100.00				88.28

My Naive Bayes implementation had a much higher accuracy of 96% on the test set. The perceptron is easily able to completely seperate the training data and get 100% accuracy on it, but it seems to generalize poorly compared to Naive Bayes.